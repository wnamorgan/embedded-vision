#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 620
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[a4paper,
            bindingoffset=0.2in,
            left=1in,
            right=1in,
            top=1in,
            bottom=1in,
            footskip=.25in]{geometry} 
\usepackage{afterpage}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{listings}
\usepackage{physics}
\usepackage{placeins}
\usepackage{xcolor}
\renewcommand{\baselinestretch}{1.5}
\DeclareMathOperator{\hr}{hr}
\DeclareMathOperator{\m}{m}
\DeclareMathOperator{\g}{g}
\DeclareMathOperator{\Hz}{Hz}
\DeclareMathOperator{\pixel}{pixel}
\DeclareMathOperator{\mm}{mm}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}


\usepackage{tcolorbox,url}
\tcbuselibrary{skins,xparse,listings}

\newtcblisting{ubuntu}{colback=violet!50!black, 
colupper=white,colframe=gray!65!black,listing only, listing options={style=tcblatex,language=sh,escapeinside=||,}, 
title={\textcolor{orange}{\Huge{$\bullet$}}{\textcolor{gray}{\Huge{$\bullet\bullet$}}}}, 
every listing line={\textcolor{lime}{\small\ttfamily\bfseries root@ubuntu{\textcolor{white}:}\textcolor{cyan!60}{$^{\sim}$}{\textcolor{white}\#} }}} 
\newlength{\Lind} 
\setbox0\hbox{\small\ttfamily\bfseries root@} 
\setlength{\Lind}{\the\dimexpr\wd0+1pt} 
\newcommand{\LR}{\\\hspace*{-\Lind}} 


\usepackage[colorlinks=true,linkcolor=blue,urlcolor=purple,bookmarksopen=true,pdfstartview=FitH]{hyperref}
\usepackage[open,openlevel=1]{bookmark}
\end_preamble
\use_default_options true
\maintain_unincluded_children no
\language american
\language_package default
\inputencoding utf8
\fontencoding auto
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement class
\float_alignment class
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_formatted_ref 0
\use_minted 0
\use_lineno 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tablestyle default
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content true
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\docbook_mathml_prefix 1
\end_header

\begin_body

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The focus on these notes is object detection,
 which has unique tools and challenges,
 different from related
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
footnote{Tasks such as image segmentation,
 semantic segmentation,
 instance segmentation,
 and panoptic segmentation are discussed in the YouTube link.}
\end_layout

\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{https://youtu.be/5QUmlXBb0MY?si=yueLdaCufC5Iye6u}{image processing}
\end_layout

\end_inset

 tasks,
 including the closely related task of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{https://learnopencv.com/object-tracking-using-opencv-cpp-python/}{object tracking}
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsection
Objective
\end_layout

\begin_layout Standard
The objective is a run real-time image processing pipeline on:
 (1) a local linux desktop with a GPU;
 (2) on Orin Nano;
 (3) a 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{https://hailo.ai/products/ai-accelerators/hailo-8-ai-accelerator/}{Hailo-8}
\end_layout

\end_inset

 GPU accelerator connected to an edge device.
 
\end_layout

\begin_layout Subsection
Quick Introduction to YOLO
\end_layout

\begin_layout Standard
YOLO is maintained by ultralytics.
 There are several 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb}{google colab}
\end_layout

\end_inset

 that allow users to walk through basic image processing pipelines.
 Google Colab is an online platform that allows users access to a clean operating system and GPU accelerator,
 as demonstrated in this 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{https://youtu.be/ZN3nRZT7b24?si=7nzLLS0xesmjcSIu}{YouTube}
\end_layout

\end_inset

 video.
\end_layout

\begin_layout Subsection
Google Colab
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{https://colab.research.google.com/}{Google Colab}
\end_layout

\end_inset

 is tool that allows users to interact with a remote system,
 including GPU,
 through a Jupyter Notebook interface.
 A good over of Google Colab is provided in this 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{https://youtu.be/FXKMmilL70w?si=zAgMXX4y6rEp5QEQ}{YouTube}
\end_layout

\end_inset

 video.
\end_layout

\begin_layout Section
Image Collection,
 Labeling and Storage
\end_layout

\begin_layout Standard
write me
\end_layout

\begin_layout Subsection
Online Image Storage with Hugging Face
\end_layout

\begin_layout Standard
Images can be stored locally of course,
 but this creates multiple copies and prevents general access.
 They can also be stored online with paid private sources like Amazon S3,
 or with free public sources like hugging face.
 You can make an account with 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{https://huggingface.co/join}{huggingface}
\end_layout

\end_inset

 and your username is used to push and pull image sets.
 While there is a command line interface,
 it is easiest to create a dataset through hugging face web interface.
 Here is a typical workflow.
 First,
 zip an image setup as follows.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=bash,style=mystyle]   
\end_layout

\begin_layout Plain Layout

$ zip -r data.zip *
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset

Note,
 you should be in a folder that has the following items:
 (1) a folder called images that contains all images;
 (2) a folder called labels,
 with text files corresponding to the images in the image folder;
 and (3) a small text file called classes.txt,
 which has a list of the classes that are correspond to the label ID in the labeled text files.
 Then,
 create a new data set,
 e.g.,
 
\begin_inset Quotes eld
\end_inset

coins
\begin_inset Quotes erd
\end_inset

 through the hugging face web interface,
 and upload the files.
 This will create an online git repository with our data.
 Now,
 you can access an unzip this data as follows.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=bash,style=mystyle]   
\end_layout

\begin_layout Plain Layout

$ wget https://huggingface.co/datasets/wnamorgan/<dataset>/resolve/main/data.zip
\end_layout

\begin_layout Plain Layout

$ unzip data.zip
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset

You can install the python datasets package,
 to interact with hugging face,
 but using wget and unzip is a bit more streamlined.
\end_layout

\begin_layout Subsubsection
Repository Interface
\end_layout

\begin_layout Standard
More advanced use cases will warrant useage of the repository interface.
 In your user account you can get the URL of the git repository of interest and simply clone.
 However,
 only a link will be included for the data in the resulting clone.
 You must use lfs to pull the binary data,
 which is installed as follows.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=bash,style=mystyle]   
\end_layout

\begin_layout Plain Layout

$ sudo apt-get update
\end_layout

\begin_layout Plain Layout

$ sudo apt-get install git-lfs
\end_layout

\begin_layout Plain Layout

$ git lfs install
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset

Next,
 go into the repository and issue the following command.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=bash,style=mystyle]   
\end_layout

\begin_layout Plain Layout

$ git lfs pull
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset

This will replace the low byte count link with the actual zip file.
 You can now unzip the data,
 make changes,
 zip it back up and then commit.
 If this is the first commit you have done with hugging face,
 you will need to generate a key via the web interface to have terminal write access your repositories.
 You can now start the get credential helper which will store the token you enter when you push.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=bash,style=mystyle]   
\end_layout

\begin_layout Plain Layout

$ git config --global credential.helper store
\end_layout

\begin_layout Plain Layout

$ git push
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset

This will require you to enter your username and then the token for the password.
 Further commits will be automatic without need of a password.
 
\end_layout

\begin_layout Section
Software
\end_layout

\begin_layout Standard
Key Softrware elemetns,
 including learning frameworks will be discussed here.
\end_layout

\begin_layout Section
Python
\end_layout

\begin_layout Standard
Python Packages will be briefly discussed here as well as relevant packages in various packages.
\end_layout

\begin_layout Subsection
OpenCV
\end_layout

\begin_layout Standard
OpenCV is a fast C++/Python vision library for image I/O and classical CV,
 plus a lightweight inference runtime via cv2.dnn.
 You don’t train models in it,
 but you can run exported ONNX/other models,
 which is handy when you want a single dependency that handles camera input,
 preprocessing,
 ONNX inference,
 and rendering on an embedded box.
 Basic OpenCV can be installed with PIP by the following.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=bash,style=mystyle]   
\end_layout

\begin_layout Plain Layout

$ pip install opencv-python
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset

For the main package with additional "contrib" modules (often includes more advanced features).
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=bash,style=mystyle]   
\end_layout

\begin_layout Plain Layout

$ pip install opencv-contrib-python
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset

 Key openCV configuration parameters can be inspected as follows.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
baselinestretch}{1.0}
\end_layout

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,
 style=mystyle]
\end_layout

\begin_layout Plain Layout

import cv2
\end_layout

\begin_layout Plain Layout

print(f"cv2 Version = {cv2.__version__}")
\end_layout

\begin_layout Plain Layout

print(cv2.cuda.getCudaEnabledDeviceCount())
\end_layout

\begin_layout Plain Layout

print(cv2.getBuildInformation())
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting} 
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
baselinestretch}{1.5}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
PyTorch
\end_layout

\begin_layout Standard
PyTorch is a deep-learning framework for Python that lets you build,
 train,
 and run neural nets using tensors and automatic differentiation.
 Its main users are researchers,
 ML engineers,
 and applied developers who want to prototype quickly and still be able to ship real systems.
 It stands out because it runs “eagerly” by default (code executes line by line like normal Python),
 which makes debugging and iterating on models straightforward.
 The ecosystem adds pieces like torchvision (vision models,
 datasets,
 transforms) and torch.utils.data (data pipelines),
 so you don’t have to write all the boilerplate yourself.
 In a typical image pipeline,
 you read frames with something like OpenCV,
 convert them to tensors,
 apply resizing/normalization transforms,
 then feed them into a PyTorch model for classification,
 detection,
 or segmentation.
 The model outputs logits,
 bounding boxes,
 or masks,
 which you post-process (thresholding,
 NMS,
 coordinate transforms) and then pass to tracking,
 control,
 or UI code.
 For training,
 PyTorch gives you optimizers,
 loss functions,
 and GPU support,
 so you can run large batches and modern architectures without reinventing anything.
 For deployment,
 you usually export the trained model to ONNX,
 TorchScript,
 or a vendor-specific format (TensorRT,
 TFLite,
 etc.) that runs on your edge hardware.
 In short,
 PyTorch is the training and inference engine in the middle of the image pipeline;
 other tools handle getting pixels in and out,
 but PyTorch does the learned mapping from images to useful decisions.
\end_layout

\begin_layout Subsection
Tensor Flow
\end_layout

\begin_layout Standard
TensorFlow is a deep-learning framework,
 typically used via its Keras API,
 for defining,
 training,
 and deploying neural networks across servers,
 mobile,
 and embedded devices.
 Its main users are ML engineers and teams who want an ecosystem that covers everything from data pipelines to serving,
 rather than just a modeling library.
 Compared with PyTorch,
 TensorFlow usually has a steeper learning curve:
 you often deal with more configuration knobs (graph vs eager modes historically,
 distribution strategies,
 SavedModel formats,
 TF data pipelines) and more explicit control over deployment details.
 You define models in Keras (sequential,
 functional,
 or subclassed),
 train them with built-in optimizers and losses,
 and use GPUs/TPUs when you need scale,
 but you’re generally more exposed to “meta-level” choices about training setup and export options than in Ultralytics or other opinionated toolkits.
 In image pipelines,
 TensorFlow fits as the core model engine while tf.data or OpenCV handle reading,
 resizing,
 augmenting,
 and batching images before feeding tensors into the network.
 The model outputs probabilities,
 boxes,
 or masks;
 you then run your own post-processing (thresholds,
 NMS,
 coordinate transforms) before passing results to tracking,
 control logic,
 or UI.
 TensorFlow’s edge angle is TensorFlow Lite:
 you convert trained models to .tflite (often quantized) and run them on phones,
 MCUs,
 or NPUs where a TFLite runtime exists.
 On the server side,
 you typically put the trained model behind TensorFlow Serving or a custom Python/HTTP service.
 Overall,
 TensorFlow is less “plug-and-play” than something like Ultralytics YOLO,
 but it gives you a unified,
 production-oriented stack if you’re willing to manage the extra complexity.
\end_layout

\begin_layout Subsection
Ultralytics and YOLO
\end_layout

\begin_layout Standard
Ultralytics is a company and a software package that develops and maintains several versions of the YOLO models.
 Ultralytics offers the tools and framework to easily train,
 use,
 and deploy these models for various computer vision tasks.
 The ultralytics package can be installed as follows.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=bash,style=mystyle]   
\end_layout

\begin_layout Plain Layout

$ pip install ultralytics
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset

While Ultralytics is a generic training/inference engine,
 YOLO is the primary use case for the package.
 YOLO (You Only Look Once) refers to a family of object detection models known for their real-time performance.
 All recent YOLO models are built on PyTorch.
 YOLO models are denoted by a version number (e.g.,
 YOLO8 and YOLO11) followed by a letter:
 (n) Nano models with about 2.6M parameters;
 (s) Small models;
 (m) Medium models with about 20.1M parameters;
 (l) Large with 25.3M parameters;
 and (x),
 or Extra Large models which achieve maximum performance.
 
\end_layout

\begin_layout Subsection
TorchVision
\end_layout

\begin_layout Standard
If you have an NVIDIA GPU,
 you can install the GPU-enabled version of PyTorch by issuing the following command.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=bash,style=mystyle]   
\end_layout

\begin_layout Plain Layout

$ pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Section
GPU Diagnostics
\end_layout

\begin_layout Standard
This section will demonstrate ways of interrogating GPU availability and useage.
\end_layout

\begin_layout Subsection
Ultralytics
\end_layout

\begin_layout Standard
If using the python Ultralytics package,
 the presence of the GPU (and other resources) as follows (at a python prompt).
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=bash,style=mystyle]   
\end_layout

\begin_layout Plain Layout

$ import ultralytics
\end_layout

\begin_layout Plain Layout

$ ultralytics.checks()
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
NVIDIA Edge Devices 
\end_layout

\begin_layout Standard
Jetson users can employ the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{https://github.com/rbonghi/jetson_stats}{jetson-stats}
\end_layout

\end_inset

 for monitoring and control of Jetson components,
 including the GPU.
 Simply type 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{https://www.youtube.com/watch?v=2cc6Irm0WuA}{jtop}
\end_layout

\end_inset

 at the command line to run jetson-stats.
\end_layout

\begin_layout Section
NVIDIA Edge Devices
\end_layout

\begin_layout Standard
On NVIDIA platforms it is useful to distinguish between how networks are trained and how they are executed at the edge.
 Training is typically done on a workstation or server using general-purpose GPU frameworks such as PyTorch (often with torchvision) or TensorFlow/Keras,
 which expose CUDA-accelerated tensors and automatic differentiation but do not assume any particular deployment target.
 Once a model is trained and validated,
 it is exported from the framework’s native format (for example,
 as an ONNX file or a PyTorch checkpoint) and then adapted to the edge device using hardware-specific tools that optimize inference latency,
 power,
 and memory use.
\end_layout

\begin_layout Standard
In a typical workflow,
 training happens off-board in a framework such as PyTorch (optionally with torchvision for datasets,
 transforms,
 and pretrained models) or TensorFlow/Keras,
 and models are saved in the framework’s native format before being exported to an interchange format like ONNX.
 At deployment time,
 many edge systems do not run these full training frameworks;
 instead,
 they often rely on lightweight components such as OpenCV (or NVIDIA image libraries) for camera I/O and GPU- or CPU-accelerated preprocessing,
 a compiled inference engine such as TensorRT (fed from an ONNX model) for the forward pass,
 and then OpenCV or custom code again for postprocessing (non-maximum suppression,
 coordinate transforms,
 overlays,
 tracking,
 and control logic).
 In other words,
 PyTorch/torchvision (and similar tools) are primarily training-time utilities,
 while OpenCV and TensorRT (and their NVIDIA counterparts) are primarily run-time tools for efficient pre-/post-processing and inference on the edge.
 When using Ultralytics YOLO,
 the same Python package can also act as a deployment wrapper around TensorRT,
 handling preprocessing,
 engine execution,
 and YOLO-specific decoding/NMS on the device.
\end_layout

\begin_layout Subsection
Deployment with TensorRT
\end_layout

\begin_layout Standard
TensorRT is NVIDIA’s inference engine and optimizer for running trained neural networks as efficiently as possible on their GPUs,
 including Jetson and Orin devices.
 It acts as a compiler that takes an ONNX model and produces a TensorRT engine (.engine or .plan):
 a hardware-specific,
 binary-optimized model with fused layers and reduced precision (FP16/INT8).
 You do not train in TensorRT;
 you only deploy and optimize an already-trained network.
 In an image pipeline it typically sits in the middle:
 camera + OpenCV preprocessing → TensorRT engine → postprocessing (NMS,
 coordinate transforms,
 temporal filtering) → tracking,
 control,
 or UI.
 In summary,
 TensorRT is the step that turns a generic model into one that runs as fast as possible on NVIDIA hardware.
\end_layout

\begin_layout Subsection
TensorRT Deployment
\end_layout

\begin_layout Standard
TensorRT can be used directly via its C++ or Python APIs,
 but doing so typically requires detailed knowledge of the model architecture and output format,
 along with custom implementations of preprocessing,
 decoding,
 and non-maximum suppression.
 This provides maximum control but demands significant development time and model-specific insight.
 For many use cases it is more expedient to use a higher-level Python package such as Ultralytics YOLO,
 which wraps TensorRT and automatically handles these model-specific steps,
 from image normalization and batching to YOLO decoding and NMS.
 In this way,
 developers retain the performance advantages of TensorRT while avoiding much of the glue code that a direct integration would require.
 In this work,
 TensorRT engines produced from YOLO models are invoked through the Ultralytics runtime rather than via raw TensorRT API calls,
 trading some low-level control for a simpler and more maintainable edge deployment.
\end_layout

\begin_layout Subsubsection
Engine Creation
\end_layout

\begin_layout Standard
TensorRT’s trtexec tool can be used to build an engine without any additional framework dependencies.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=bash,style=mystyle,
 caption = Offline build step:
 PyTorch checkpoint → TensorRT engine]   
\end_layout

\begin_layout Plain Layout

$ trtexec --onnx=model.onnx --saveEngine=model.engine --fp16
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting}
\end_layout

\end_inset

 You can also create the RT engine file with packages like Ultralytics.
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
baselinestretch}{1.0}
\end_layout

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,
 style=mystyle,
 caption=Offline build step:
 PyTorch checkpoint (.pt) → TensorRT engine (.engine) ,label={lsting:
 pytorch to engine}]
\end_layout

\begin_layout Plain Layout

from ultralytics import YOLO
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

model = YOLO("yolo11n.pt")
\end_layout

\begin_layout Plain Layout

model.export(format="engine",
 device="cuda",
 half=True)  # produces yolo11n.engine
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting} 
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
baselinestretch}{1.5}
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
TensorRT Runtime Inference
\end_layout

\begin_layout Standard
To streampline deployment,
 it is conveinent to use inference piplines built ontop of TensorRT.
 Two good choices are 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{https://github.com/dusty-nv/jetson-inference}{NVIDIA's DNN library}
\end_layout

\end_inset

 or 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
href{https://docs.ultralytics.com/guides/nvidia-jetson/#use-tensorrt-on-nvidia-jetson}{Ultralytics}
\end_layout

\end_inset

.
 The focus here is on Ultralytics,
 since the pipeline it provides can readily be adapted to other edge devices that do not use TensorRT..
 The basic inference process is illustrated in Listing 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{lsting:
 yolo and tensorRT}
\end_layout

\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
baselinestretch}{1.0}
\end_layout

\begin_layout Plain Layout


\backslash
begin{lstlisting}[language=Python,
 style=mystyle,
 caption=Offline build step:
 PyTorch checkpoint (.pt) → TensorRT inference,
 label={lsting:
 yolo and tensorRT}]
\end_layout

\begin_layout Plain Layout

from ultralytics import YOLO
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Load a YOLO11n PyTorch model
\end_layout

\begin_layout Plain Layout

model = YOLO("yolo11n.pt")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Export the model to TensorRT
\end_layout

\begin_layout Plain Layout

model.export(format="engine")  # creates 'yolo11n.engine'
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Load the exported TensorRT model
\end_layout

\begin_layout Plain Layout

trt_model = YOLO("yolo11n.engine")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Run inference
\end_layout

\begin_layout Plain Layout

results = trt_model("https://ultralytics.com/images/bus.jpg")
\end_layout

\begin_layout Plain Layout


\backslash
end{lstlisting} 
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
baselinestretch}{1.5}
\end_layout

\end_inset


\end_layout

\end_body
\end_document
